{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('CA1-Classification-Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPUTING AND CLEANING OF DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "data.isnull().sum()\n",
    "\n",
    "# Create an imputer object\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Impute the missing values\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "# Convert the imputed data back to a pandas DataFrame\n",
    "df = pd.DataFrame(imputed_data, columns=data.columns)\n",
    "\n",
    "# Print the imputed DataFrame\n",
    "print(df)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = df.drop('Quality', axis=1)\n",
    "y = df['Quality']\n",
    "\n",
    "df =df.dropna(subset=['Trihalomethanes']) # dropped as it is less than 5% of data\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#training model\n",
    "LRmodel = LogisticRegression()\n",
    "LRmodel.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "pred_LR = LRmodel.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_LR = accuracy_score(y_test, pred_LR)\n",
    "conf_matrix_LR = confusion_matrix(y_test, pred_LR)\n",
    "classification_rep_LR = classification_report(y_test, pred_LR)\n",
    "\n",
    "print(f'Logistic Regression Accuracy: {accuracy_LR}')\n",
    "print(f'Logistic Regression Classification Report:\\n{classification_rep_LR}')\n",
    "\n",
    "# Confusion Matrix Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_LR, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance\n",
    "coefficients = LRmodel.coef_[0]\n",
    "feature_importance = dict(zip(X.columns, coefficients))\n",
    "\n",
    "# Plotting feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(X.columns, coefficients, color='green')\n",
    "plt.title('Feature Importance for Logistic Regression')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECISION TREE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "tree_classifier = DecisionTreeClassifier(max_depth = 4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_tree = tree_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "conf_matrix_tree = confusion_matrix(y_test, y_pred_tree)\n",
    "classification_rep_tree = classification_report(y_test, y_pred_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Decision Tree Accuracy: {accuracy_tree}')\n",
    "print(f'Decision Tree Confusion Matrix:\\n{conf_matrix_tree}')\n",
    "print(f'Decision Tree Classification Report:\\n{classification_rep_tree}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Assuming y_test and y_pred_tree are pandas Series or NumPy arrays\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_tree)\n",
    "\n",
    "# Display the confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2'], \n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "plt.title('Decision Tree Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance for Decision Tree\n",
    "feature_importance = tree_classifier.feature_importances_\n",
    "print(\"Feature Importance (Decision Tree):\")\n",
    "print(dict(zip(X.columns, feature_importance)))\n",
    "\n",
    "# Feature Importance Bar Chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(X.columns, feature_importance)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Decision Tree Feature Importance')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DUMMY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Create a DummyClassifier predicting the majority class\n",
    "dummy_classifier = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "# Train the DummyClassifier on the training set\n",
    "dummy_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "pred_dummy = dummy_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy_dummy = accuracy_score(y_test, pred_dummy)\n",
    "\n",
    "# Print the accuracy of the dummy classifier\n",
    "print(f'Dummy Classifier Accuracy: {accuracy_dummy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are your training and testing data\n",
    "\n",
    "# Create a KNeighborsClassifier\n",
    "knn_classifier = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search for the best hyperparameters\n",
    "param_grid = {'n_neighbors': range(3, 15)}\n",
    "\n",
    "# Use GridSearchCV to search for the best hyperparameters\n",
    "grid_search = GridSearchCV(knn_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best K value from the grid search\n",
    "best_k_value = grid_search.best_params_['n_neighbors']\n",
    "\n",
    "# Create a KNeighborsClassifier with the best K value\n",
    "best_knn_classifier = KNeighborsClassifier(n_neighbors=best_k_value)\n",
    "\n",
    "# Fit the model with the best hyperparameters on the entire training set\n",
    "best_knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "pred_knn = best_knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy_knn = accuracy_score(y_test, pred_knn)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm_knn = confusion_matrix(y_test, pred_knn)\n",
    "\n",
    "# Generate classification report\n",
    "class_report_knn = classification_report(y_test, pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best K value and accuracy\n",
    "print(\"Best K value:\", best_k_value)\n",
    "print(\"Accuracy with best K:\", accuracy_knn)\n",
    "\n",
    "\n",
    "# Print the classification report\n",
    "print(\"KNN Classification Report:\")\n",
    "print(class_report_knn)\n",
    "\n",
    "\n",
    "# Plotting the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_knn, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=best_knn_classifier.classes_, yticklabels=best_knn_classifier.classes_)\n",
    "plt.title('KNN Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Bar graph comparing accuracy of the best KNN model with other classifiers including the dummy classifier\n",
    "accuracies = [accuracy_LR, accuracy_tree, accuracy_knn, accuracy_dummy]\n",
    "models = ['Logistic Regression', 'Decision Tree', 'K-Nearest Neighbors', 'Dummy Classifier']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(models, accuracies, color=['blue', 'green', 'orange', 'gray'])\n",
    "plt.title('Comparison of Classification Model Accuracies')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Displaying the accuracy values on top of the bars\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v + 0.02, f'{v:.5f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
